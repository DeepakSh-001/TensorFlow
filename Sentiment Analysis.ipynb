{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e47b815b-396a-4c14-a18d-cf14d553e8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbe1357-57e5-4e58-8212-fb52c95e88e9",
   "metadata": {},
   "source": [
    "Define Training Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9855e85-6c49-4bf7-bd47-e67e363ccd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_data = [\"It is a sunny day\",\"It is a cloudy day\",\"Will it rain today or not?\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dcf855-e8bb-4aec-b391-2eddf4050fbe",
   "metadata": {},
   "source": [
    "Set up tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a76f42e5-03dc-4046-979e-160bcf515bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words =100)\n",
    "tokenizer.fit_on_texts(Train_data)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71eba0c3-34bc-4580-93b8-e6aaa5534fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'it': 1,\n",
       " 'is': 2,\n",
       " 'a': 3,\n",
       " 'day': 4,\n",
       " 'sunny': 5,\n",
       " 'cloudy': 6,\n",
       " 'will': 7,\n",
       " 'rain': 8,\n",
       " 'today': 9,\n",
       " 'or': 10,\n",
       " 'not': 11}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec61922-602e-4842-a578-f22d8a18b2c6",
   "metadata": {},
   "source": [
    "Create Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3811cc8-7632-484b-b46a-1d74982f7d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(Train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "048a9984-1b27-451e-b470-306944dc19ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 5, 4], [1, 2, 3, 6, 4], [7, 1, 8, 9, 10, 11]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bd42cb-80da-460e-a601-c957a0ce1ed7",
   "metadata": {},
   "source": [
    "These sequences will not going to have new words.It will just leave that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1666da55-f806-49d2-a9ad-191def5b5fee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 4, 2]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sentences = [\"Sunny day is fine\"]\n",
    "new_sequences = tokenizer.texts_to_sequences(new_sentences)\n",
    "new_sequences\n",
    "#It will only show sequences of those words which are present in my training set.\n",
    "# To deal with this we use oov_token, which gives encoding 1 to out of vocabulary words.\n",
    "# tokenizer = Tokenizer(num_words =100, oov_token = \"<oov>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2805eb82-75d6-4f16-b360-22c897905ce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 5, 4], [1, 2, 3, 6, 4], [7, 1, 8, 9, 10, 11]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training set sentences should have same number of words - can be achieved by padding or truncation\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "padded_seq = pad_sequences(sequences)\n",
    "padded_seq\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bed6ec16-522d-4fa8-bb6f-a758e6d107aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3,  5,  4],\n",
       "       [ 0,  1,  2,  3,  6,  4],\n",
       "       [ 7,  1,  8,  9, 10, 11]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50322804-3010-411d-8044-68c16ac21b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3,  5,  4,  0],\n",
       "       [ 1,  2,  3,  6,  4,  0],\n",
       "       [ 7,  1,  8,  9, 10, 11]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#customising the padded_seq\n",
    "padded_new = pad_sequences(sequences,padding = \"post\") #maxlen = 5, truncating = \"post\"\n",
    "padded_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c62a05-201f-4080-8a81-42714b0c8c66",
   "metadata": {},
   "source": [
    "#Word Embedding - Dense or numerical vector representations of words\n",
    "They capture semantic meaning: \"king\" and \"queen\" will have similar embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af6d209e-dabf-43df-9538-9337d7725268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.19.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95f91644-fe6c-4550-9c44-b4e0c02c7487",
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading imdb dataset\n",
    "data,info = tfds.load(\"imdb_reviews\", with_info =True, as_supervised = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f3b5bf9-541c-4c45-8435-3333d62c7512",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='imdb_reviews',\n",
       "    full_name='imdb_reviews/plain_text/1.0.0',\n",
       "    description=\"\"\"\n",
       "    Large Movie Review Dataset. This is a dataset for binary sentiment\n",
       "    classification containing substantially more data than previous benchmark\n",
       "    datasets. We provide a set of 25,000 highly polar movie reviews for training,\n",
       "    and 25,000 for testing. There is additional unlabeled data for use as well.\n",
       "    \"\"\",\n",
       "    config_description=\"\"\"\n",
       "    Plain text\n",
       "    \"\"\",\n",
       "    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
       "    data_dir='C:\\\\Users\\\\Manny\\\\tensorflow_datasets\\\\imdb_reviews\\\\plain_text\\\\1.0.0',\n",
       "    file_format=tfrecord,\n",
       "    download_size=80.23 MiB,\n",
       "    dataset_size=129.83 MiB,\n",
       "    features=FeaturesDict({\n",
       "        'label': ClassLabel(shape=(), dtype=int64, num_classes=2),\n",
       "        'text': Text(shape=(), dtype=string),\n",
       "    }),\n",
       "    supervised_keys=('text', 'label'),\n",
       "    disable_shuffling=False,\n",
       "    nondeterministic_order=False,\n",
       "    splits={\n",
       "        'test': <SplitInfo num_examples=25000, num_shards=1>,\n",
       "        'train': <SplitInfo num_examples=25000, num_shards=1>,\n",
       "        'unsupervised': <SplitInfo num_examples=50000, num_shards=1>,\n",
       "    },\n",
       "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
       "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
       "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
       "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
       "      month     = {June},\n",
       "      year      = {2011},\n",
       "      address   = {Portland, Oregon, USA},\n",
       "      publisher = {Association for Computational Linguistics},\n",
       "      pages     = {142--150},\n",
       "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
       "    }\"\"\",\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "219341b8-53c9-43f6-8cda-039d5956d928",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = data['train'], data['test']\n",
    "train_sentences = []\n",
    "test_sentences = []\n",
    "train_labels = []\n",
    "test_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d94be1cc-c908-4a31-b59d-7fe66bfe2e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate over train data to extract sentences & labels\n",
    "#.numpy() converts the tf.Tensor to a NumPy object (i.e., raw bytes).\n",
    "#.decode('utf8') converts those bytes to a string.\n",
    "for sentence,label in train_data:\n",
    "    train_sentences.append(str(sentence.numpy().decode('utf8')))\n",
    "    train_labels.append(label.numpy())\n",
    "\n",
    "for sentence,label in test_data:\n",
    "    test_sentences.append(str(sentence.numpy().decode('utf8')))\n",
    "    test_labels.append(label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12c7498b-a22f-4e6e-8654-67978ac0d73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5fe866-3e46-48c4-a660-aecaa6748e83",
   "metadata": {},
   "source": [
    "### Data Preparation - Setting up tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f42df17-1288-4b04-93f7-0fcf0c1841bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "oov_tok = '<oov>'\n",
    "embedding_dim = 15\n",
    "max_length = 150\n",
    "truncating = 'post'\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words =vocab_size, oov_token = oov_tok)\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "train_seq = tokenizer.texts_to_sequences(train_sentences)\n",
    "train_padded = pad_sequences(train_seq, maxlen = max_length, truncating = truncating)\n",
    "\n",
    "test_seq = tokenizer.texts_to_sequences(test_sentences)\n",
    "test_padded = pad_sequences(test_seq, maxlen = max_length, truncating = truncating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b57d1ffd-82dc-4e4a-95d4-4b209c2c8b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0   11   26   75  571\n",
      "    6  805 2354  313  106   19   12    7  629  686    6    4 2219    5\n",
      "  181  584   64 1454  110 2263    3 3951   21    2    1    3  258   41\n",
      " 4677    4  174  188   21   12 4078   11 1578 2354   86    2   20   14\n",
      " 1907    2  112  940   14 1811 1340  548    3  355  181  466    6  591\n",
      "   19   17   55 1817    5   49   14 4044   96   40  136   11  972   11\n",
      "  201   26 1046  171    5    2   20   19   11  294    2 2155    5   10\n",
      "    3  283   41  466    6  591    5   92  203    1  207   99  145 4382\n",
      "   16  230  332   11 2486  384   12   20   31   30]\n",
      "? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? i have been known to fall asleep during films but this is usually due to a combination of things including really tired being warm and comfortable on the <oov> and having just eaten a lot however on this occasion i fell asleep because the film was rubbish the plot development was constant constantly slow and boring things seemed to happen but with no explanation of what was causing them or why i admit i may have missed part of the film but i watched the majority of it and everything just seemed to happen of its own <oov> without any real concern for anything else i cant recommend this film at all\n"
     ]
    }
   ],
   "source": [
    "reverse_word_index = dict([(value,key) for (key,value) in word_index.items()])\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i,'?') for i in text ])\n",
    "\n",
    "print(train_sentences[1])\n",
    "print(train_padded[1])\n",
    "print(decode_review(train_padded[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90abf21d-17e5-4ce9-a25f-67fbeb8bc699",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Define the neural network with embedding layer\n",
    "1. Use the Sequential API\n",
    "2. Add an embedding input layer of input size equal to vocab size\n",
    "3. Add a flatten layer, and 2 dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d980302a-980f-4d67-bfa3-9a97380eca5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">150,000</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2250</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)                   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">13,506</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │               <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m15\u001b[0m)             │         \u001b[38;5;34m150,000\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2250\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)                   │          \u001b[38;5;34m13,506\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │               \u001b[38;5;34m7\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">163,513</span> (638.72 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m163,513\u001b[0m (638.72 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">163,513</span> (638.72 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m163,513\u001b[0m (638.72 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size,embedding_dim),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(6, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')])\n",
    "# compile the model with loss function, optimizer, & metrics\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.build(input_shape=(None, max_length))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d531c30-0483-4f3d-ba70-61a47e8a92a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d93f895d-423a-49fd-8422-b4cddf0a281f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa8f367a-3344-4cca-b310-baa796029aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "(25000, 150) (25000,)\n"
     ]
    }
   ],
   "source": [
    "print(type(train_padded), type(train_labels))\n",
    "print(np.shape(train_padded), np.shape(train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18121e89-1d2f-45ae-b11f-7ca22902c71d",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d79f0aed-0791-436d-abf0-7a65fff75f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.6318 - loss: 0.6110 - val_accuracy: 0.8362 - val_loss: 0.3681\n",
      "Epoch 2/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9078 - loss: 0.2485 - val_accuracy: 0.8258 - val_loss: 0.3940\n",
      "Epoch 3/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9711 - loss: 0.1143 - val_accuracy: 0.8220 - val_loss: 0.4594\n",
      "Epoch 4/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9942 - loss: 0.0355 - val_accuracy: 0.8112 - val_loss: 0.5626\n",
      "Epoch 5/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9980 - loss: 0.0118 - val_accuracy: 0.8112 - val_loss: 0.6362\n",
      "Epoch 6/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9997 - loss: 0.0045 - val_accuracy: 0.8101 - val_loss: 0.6916\n",
      "Epoch 7/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9998 - loss: 0.0029 - val_accuracy: 0.8160 - val_loss: 0.7435\n",
      "Epoch 8/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 6.1296e-04 - val_accuracy: 0.8178 - val_loss: 0.7814\n",
      "Epoch 9/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 3.4147e-04 - val_accuracy: 0.8190 - val_loss: 0.8154\n",
      "Epoch 10/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 2.0826e-04 - val_accuracy: 0.8184 - val_loss: 0.8551\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1709de1aab0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_padded = train_padded[:25000]\n",
    "num_epochs = 10\n",
    "model.fit(train_padded,train_labels,epochs = num_epochs,validation_data = (test_padded,test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bb6e05-1401-4dca-9737-775d8276c81c",
   "metadata": {},
   "source": [
    "### Derieving Weights from embedding layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6932dbd-04f3-4540-83a9-992ad83da53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 15)\n",
      "[[-0.00970541  0.04358897  0.05723758 ...  0.05473125 -0.00485928\n",
      "  -0.02095112]\n",
      " [ 0.05941346  0.07290929  0.07515416 ...  0.06891727  0.01599611\n",
      "  -0.07985418]\n",
      " [-0.01457466  0.13918164  0.14539869 ...  0.17153877  0.00908752\n",
      "  -0.01000997]\n",
      " ...\n",
      " [-0.13791926 -0.11145771 -0.00449798 ... -0.02784112 -0.15250528\n",
      "  -0.07114182]\n",
      " [-0.04097483  0.00964436  0.05315474 ...  0.02515131 -0.16377777\n",
      "   0.07427062]\n",
      " [-0.07930727 -0.0995072   0.01404452 ... -0.14835927 -0.05205519\n",
      "  -0.19359782]]\n"
     ]
    }
   ],
   "source": [
    "ll = model.layers[0]\n",
    "\n",
    "#extracting learned weights\n",
    "weights = ll.get_weights()[0]\n",
    "print(weights.shape)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb26b6d7-2f32-466c-84a9-48b7ae2ab7cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
